{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tilecoding import TileCoder \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.5\n",
    "initial_lr = 0.5\n",
    "min_lr = 0.1\n",
    "gamma =0.99\n",
    "max_steps=200\n",
    "epi_num=5000\n",
    "max_epsilon=1.0\n",
    "min_epsilon=0.01\n",
    "decay_rate = (max_epsilon-min_epsilon)/epi_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "''' #  Q = defaultdict(lambda: np.zeros(2))  #\n",
    " ignored using lambda function because : Lambda functions are anonymous and cannot be referenced by name.'\n",
    " The pickle module requires functions to be defined at the top level of a module (with a name) to serialize them.'''\n",
    "def default_q_value():\n",
    "    return np.zeros(env.action_space.n)\n",
    "\n",
    "Q = defaultdict(default_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state,epsilon):\n",
    "    p=np.random.random()\n",
    "    if p<epsilon:\n",
    "        action=env.action_space.sample()\n",
    "    else:\n",
    "        action=np.argmax(Q[state])\n",
    "    return action\n",
    "\n",
    "def state_discretize(state):\n",
    "    position_step=0.6\n",
    "    velocity_step=0.75\n",
    "    angposition_step=0.075\n",
    "    angvelocity_step=0.5\n",
    "    state[0] = np.round(state[0] / position_step) * position_step\n",
    "    state[1] = np.round(state[1] / velocity_step) * velocity_step\n",
    "    state[2] = np.round(state[2] / angposition_step) * angposition_step\n",
    "    state[3] = np.round(state[3] / angvelocity_step) * angvelocity_step\n",
    "\n",
    "    rounded_state= tuple(np.clip(state,-3,3))\n",
    "    return rounded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_tiler(state):\n",
    "    # the number of tiles per dimension\n",
    "    tiles_per_dim = [8, 4, 10, 4]  # Example: 8 tiles for each of the 4 dimensions\n",
    "    \n",
    "    # the value limits for each dimension\n",
    "    value_limits = [\n",
    "        (-2.4, 2.4),       # Position limits\n",
    "        (-3.0, 3.0),       # Velocity limits\n",
    "        (-0.2095, 0.2095),   # Angle limits (in radians)\n",
    "        (-2.0, 2.0)        # Angular velocity limits\n",
    "    ]\n",
    "    \n",
    "    # Number of tilings\n",
    "    tilings = 4  # Example: 8 tilings\n",
    "    \n",
    "    # Create a TileCoder instance\n",
    "    tiler = TileCoder(tiles_per_dim, value_limits, tilings)\n",
    "    \n",
    "    # Encode the state into tiles\n",
    "    active_tiles = tiler[state]\n",
    "    \n",
    "    return tuple(active_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_control(lr):\n",
    "    episodic_rewards = []\n",
    "    episodic_lengths = []\n",
    "    epsilon = max_epsilon\n",
    "    \n",
    "    for episode in range(epi_num):\n",
    "        state, _ = env.reset()\n",
    "        state = state_tiler(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps=0\n",
    "        while not done:\n",
    "            action = epsilon_greedy(state, epsilon)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            next_state = state_tiler(next_state)\n",
    "            if done :\n",
    "                reward=-1\n",
    "            # Q-learning update\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += lr * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "                \n",
    "       \n",
    "        epsilon= max(min_epsilon,max_epsilon-(episode*decay_rate))\n",
    "        lr = max(min_lr, lr * (1 - episode / epi_num))\n",
    "        \n",
    "        episodic_rewards.append(total_reward)\n",
    "        episodic_lengths.append(steps)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episodic_rewards[-100:])\n",
    "            print(f\"Episode: {episode}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "    \n",
    "    return episodic_rewards, episodic_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards, episodic_lengths=q_learn_control(lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodic_rewards)\n",
    "plt.plot(pd.Series(episodic_rewards).rolling(200).mean())\n",
    "plt.title(\"Training Progress\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"q_table.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q, f)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_episodes=500\n",
    "test_rew=[]\n",
    "test_steps=[]\n",
    "for epiIndex in range(test_episodes):\n",
    "    state,_=env.reset()\n",
    "    state=state_tiler(state)\n",
    "    total_rew=0\n",
    "    total_steps=0\n",
    "    done = False\n",
    "    while(not done):\n",
    "        action=np.argmax(Q[state])\n",
    "\n",
    "        next_state,reward,done,truncated,_=env.step(action)\n",
    "        total_rew+=1\n",
    "        total_steps+=1\n",
    "        next_state=state_tiler(next_state)\n",
    "        state=next_state\n",
    "        if done or truncated:\n",
    "            break\n",
    "    test_rew.append(total_rew)\n",
    "    test_steps.append(total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_rew)\n",
    "plt.plot(pd.Series(test_rew).rolling(50).mean())\n",
    "plt.title(\"Testing Progress\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_steps)\n",
    "plt.plot(pd.Series(test_steps).rolling(50).mean())\n",
    "plt.title(\"Testing Progress\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Steps\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
